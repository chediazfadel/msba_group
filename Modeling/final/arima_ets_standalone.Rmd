---
title: "Modeling - ARIMA + ETS"
author: "Che Diaz Fadel"
date: "2023-11-05"
output: 
  html_document:
    number_sections: no
    toc: yes
    fig_width: 15
    fig_height: 10
    highlight: tango
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
                      fig.path = "EDA_figs/EDA_")
```

```{r}
# Load packages 
library(tidyverse)
library(lubridate)
library(zoo)
library(fpp3)
library(doParallel)
library(caret)
library(xgboost)

```

## Data preparation

```{r}
# get desired column names from EDA
fixed_cnames <- colnames(read_csv(paste0("q_data.csv")) %>%
                           mutate(men_toilet_count = NA,
                                  .after = self_check_out) %>%
                           select(-rv_fueling_positions))[c(1:39, 41, 42, 40, 43:52)]


q_data <- read_csv(paste0("qualitative_data_msba.csv")) %>%
  # Remove row index and duplicated columns
  select(-c(1, `RV Lanes Fueling Positions`, `Hi-Flow Lanes Fueling Positions`)) %>%
  # properly encode "None"
  mutate(
    across(
      where(~any(grepl("^N/?A$", ., ignore.case = TRUE))),
      ~replace(., grepl("^N/?A$", ., ignore.case = TRUE), "None")
    )
  ) %>%
  rename_with(~fixed_cnames) %>%
  relocate(site_id) %>%
  # omitting zero-variance variables
  select(-c(fDoor_count, godfathers_pizza, diesel, car_wash, 
            ev_charging, non_24_hour, self_check_out))

# Calculate standardized day id
day_id_df <- tibble(date = seq(as_date("2021-01-01"), as_date("2023-12-31"), "1 day")) %>%
  # Calculate week_id
  mutate(week_id = yearweek(date, week_start = 5) %>% format("%V") %>% as.numeric(),
         # since the first day of fiscal year 2022 is actually in 2021, special logic must be 
         # applied to identify the beginning of the year
         x = case_when(lag(week_id, default = 52) == 52 & week_id == 1 ~ 1),
         year = 2020 + rollapplyr(x, width = n(), FUN = sum, na.rm = TRUE, partial = TRUE)) %>%
  group_by(year) %>%
  mutate(day_id = row_number()) %>%
  select(-x) %>%
  ungroup()

t_series <- read_csv(paste0("t_series.csv")) %>%
  # remove missing store
  filter(site_id != 23065) %>%
  relocate(site_id, date) %>%
  arrange(site_id, date) %>%
  mutate(id = row_number(),
         .before = 1) %>%
  left_join(day_id_df %>%
              select(date, day_id), "date") %>%
  group_by(site_id) %>%
  mutate(first_day_id = first(day_id)) %>%
  ungroup() %>%
  arrange(first_day_id, site_id) %>%
  group_by(site_id) %>%
  # Encode an alternative day_id which can exist in 2 years
  mutate(day_id2 = purrr::accumulate(day_id, ~ifelse(.x < .y, .y, .y + 364)),
         date2 = as_date(as.numeric(as_date("2021-01-01")) + (day_id2 - 1))) %>%
  ungroup() %>%
  select(-c(first_day_id))

merged_data <- t_series %>%
  # Join time series and qualitative data
  left_join(q_data,
            "site_id") %>%
  arrange(site_id, date) %>%
  # create observation index variable
  group_by(site_id) %>%
  mutate(start_id = row_number(),
         .before = 2) %>%
  ungroup()

```

Maverik expressed the importance of aligning days in a standardized manner. `day_id` represents the nth day of a given year. One limitation with `day_id` is that it does not preserve order. For example, if a site opened in the last month of the year, `day_id` 200 actually occurred before `day_id` 20. `day_id2` solves this by allowing the index to span two calendar years.\

I struggled to get the models available in the `fpp3` package to behave as expected when the "index" of a created tsibble was not a native date object, so instead of using `day_id2` as the index in the tsibble, I created `date2` which corresponds directly to `day_id2` but forces the assumption that every site opened in the same year.

## Modeling

The following takes 5 random stores and fits 3 models using data starting from day 1 to day 366. In total, 5,490 models are fitted. The three model types are:\

-   ARIMA\
-   Regression with ARIMA regressors\
-   ETS\

This implementation from the `fpp3` package allows for automatic selection of each model's component parts that provide the best fit. While this is a very computationally expensive approach, it is still practical for business implementation. To speed up the process, I've elected to compute in parallel using the `doParallel` package.

```{r eval = FALSE}

# choose 5 random sites
set.seed(123)
fe_sites2 <- sample(unique(merged_data$site_id), 5)

# establish 10-core cluster
cl <- makeCluster(10)
registerDoParallel(cl)
(xtime1 <- Sys.time())

# loop over each day
fit_all <- foreach(i = 1:366,
                   .packages = c("tidyverse", "fpp3"),
                   .combine = "bind_rows") %:% 
  # loop over each sample site
  foreach(j = fe_sites2,
          .packages = c("tidyverse", "fpp3"),
          .combine = "bind_rows")  %dopar%{
            
            # subset site and training days
            ox <- merged_data %>%
              filter(site_id == j,
                     start_id <= i) %>%
              distinct(site_id, day_id, day_id2, .keep_all = TRUE) %>%
              select(start_id, site_id, day_id, day_id2, date, date2, 
                     holiday, day_of_week, day_type, ends_with("sales")) %>%
              # convert to long form
              pivot_longer(inside_sales:unleaded_sales,
                           names_to = "tvar",
                           values_to = "sales") %>%
              arrange(site_id, tvar, start_id) %>%
              # create tsibble with appropriate key and index
              as_tsibble(index = date2, key = c(site_id, tvar)) %>%
              # define models
              model(arima1 = ARIMA(sales),
                    arima2 = ARIMA(sales ~ season("week")),
                    ets = ETS(sales)) %>%
              # log iteration information
              mutate(site_id = j,
                     start_init = i,
                     .before = 1) %>%
              mable(key = c(site_id, start_init, tvar), model = c(arima1, arima2, ets))
            
            
            gc()
            
            ox
          }

stopCluster(cl)
xtime2 <- Sys.time()
xtime2 - xtime1

# create tsibble of sampled sites in format compatible with fitted models
base_all <- merged_data %>%
  filter(site_id %in% fe_sites2) %>%
  distinct(site_id, day_id, day_id2, .keep_all = TRUE) %>%
  select(start_id, site_id, day_id, day_id2, date, date2, 
         holiday, day_of_week, day_type, ends_with("sales")) %>%
  pivot_longer(inside_sales:unleaded_sales,
               names_to = "tvar",
               values_to = "sales") %>%
  arrange(site_id, tvar, start_id) %>%
  as_tsibble(index = date2, key = c(site_id, tvar))

# for each site, sales metric, and day, calculate cumulative sales and remaining sales
ppdf_all <- base_all %>%
  as_tibble() %>%
  group_by(site_id, tvar) %>%
  group_modify(~{
    lapply(.x$start_id,
           \(xx){
             .x %>%
               group_by(prd = ifelse(start_id <= xx, "pre", "post")) %>%
               summarise(sales = sum(sales)) %>%
               mutate(start_id = xx,
                      .before = 1)
           }) %>%
      list_rbind()
  }) %>%
  ungroup() %>%
  pivot_wider(names_from = prd,
              values_from = sales) %>%
  mutate(sales = pre + post)

# Make daily forecasts for each site and training start day
fc_all <- fit_all %>%
  filter(start_init <= 365) %>%
  rowwise() %>%
  group_map(~{
    si <- .x$start_init
    
    .x %>%
      mable(key = c(site_id, tvar), model = c(arima1, arima2, ets)) %>%
      forecast(base_all %>%
                 filter(start_id > si)) %>%
      left_join(base_all %>%
                  filter(start_id > si) %>%
                  select(site_id, date2, sales_obs = sales), c("site_id", "tvar", "date2")) %>%
      group_by(site_id, .model) %>% 
      # calculate performance metrics for each site/training start
      mutate(start_init = si,
             er = sales_obs - .mean,
             rmse = RMSE(sales_obs, .mean),
             mape = MAPE(er, sales_obs)) %>%
      relocate(start_init, .after = site_id) %>%
      ungroup()
    
  }) %>%
  list_rbind()

# get total year performance metrics
fc_all2 <- fc_all %>%
  filter(!is.na(.mean)) %>% 
  as_tibble() %>%
  group_by(tvar, .model, site_id, start_init) %>%
  # get annual for each sales metric, model, site, training start combination
  summarise(fc = sum(.mean)) %>%
  # join cumulative/remaning truth data
  left_join(ppdf_all %>%
              select(site_id, tvar, start_init = start_id, 
                     pre, post, sales),
            c("site_id", "start_init", "tvar")) %>%
  mutate(tpred = fc + pre,
         er = sales - tpred,
         .after = post) %>%
  relocate(er, .after = last_col()) %>%
  group_by(tvar, .model) %>%
  # aggregate metric for each sales metric and model
  arrange(tvar, start_init) %>% 
  mutate(er = sales - tpred,
         rmse = RMSE(tpred, sales),
         mae = MAE(tpred, sales),
         mape = MAPE(er, sales),
         mape_step = (abs(tpred - sales)/sales) * 100,
         # get "rolling" RMSE
         rmse_roll = sapply(start_init,
                            \(xx){RMSE(tpred[start_init >= xx], sales[start_init >= xx])}),
         # get "rolling" MAPE
         mape_roll = sapply(start_init,
                            \(xx){mean(mape_step[start_init >= xx])}),
         # get "rolling" MAE
         mae_roll = sapply(start_init,
                           \(xx){MAE(tpred[start_init >= xx], sales[start_init >= xx])})
         
  ) 

```


```{r include = FALSE}

fc_all2 <- readRDS("fc_all2.RDS")

```

```{r}

fc_all2 %>% 
  # Select training start day that match Maverik's benchmarks
  filter(start_init %in% c(14, 21, 183, 300)) %>%
  ungroup() %>%
  distinct(tvar, start_init, .model, .keep_all = TRUE) %>%
  arrange(match(tvar, c("inside_sales", "food_service", "diesel", "unleaded")), start_init, rmse) %>% 
  select(tvar, start_init, .model, rmse_roll, mape_roll, mae_roll, rmse, mape, mae) %>% 
  group_by(tvar, start_init) %>%
  # subset best performing instances
  filter(rmse_roll == min(rmse_roll) | mape_roll == min(mape_roll)) %>%
  ungroup() %>%
  arrange(match(tvar, c("inside_sales", "food_service_sales", "diesel_sales", "unleaded_sales")),
          start_init) %>%
  # make output more readable
  mutate(start_init = case_match(start_init,
                                 14 ~ "2 weeks",
                                 21 ~ "3 weeks",
                                 183 ~ "6 months",
                                 300 ~ "300 days"),
         tvar = paste(tvar, start_init)) %>%
  select(tvar, rmse_roll, mape_roll) %>%
  # prevent scientific notation output
  mutate(across(c(rmse_roll, mape_roll), ~as.character(round(., 2)))) %>%
  filter(!grepl("300", tvar))
  
```

## Results

The RMSE values for each forecast period are an improvement to Maverik's benchmarks and therefore justifies the computational expense required to yield these results. While other models can produce superior performance metrics, this particular model has much greater versatility and the ability to update models as new data is observed to produce daily forecasts. 