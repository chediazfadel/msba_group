---
title: "EDA"
author: "Che Diaz Fadel"
date: "2023-09-26"
output: 
  html_document:
    number_sections: yes
    toc: yes
    df_print: paged
    theme: darkly
    highlight: tango
    fig_width: 15
    fig_height: 10
editor_options: 
  chunk_output_type: console
---
<style type="text/css">
.main-container {
  max-width: 1100px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

options(tibble.print_max = 40,
        tibble.print_min = 24,
        width = 150,
        pillar.min_title_chars = 15,
        scipen = 999)
```

# Introduction
## Business problem 

Maverik is interested in producing more accurate financial plans and initial ROI documents for future convenience store locations. Considerable uncertainty is associated with deploying new locations in their network and being able to properly allocate resources and accurately predict profitability is crucial to the prosperity of their business. To this end, this project aims to augment veracity of financial plans and ROI documents by leveraging the store-level time series and qualitative data collected by Maverik. This will be done by employing an ensemble of forecasting and supervised regression models designed to provide daily store level sales forecasts of multiple key product categories. Success of this project will be benchmarked against Maverik's existing Naive forecasting solution and the proposed model will be tuned to minimize:\

-   Akaike Information Criterion (AIC)\
-   Root Mean Squared Error (RMSE)\
-   Mean Absolute Percentage Error (MAPE)\

Key abilities of the final must include:\

-   Calculation of daily level, seasonally sensitive forecast for each of the sales metrics defined by Maverik.\
-   Functionality to create updated forecasts given new data.\

The ability to accurately forecast store sales will enable Maverik to optimize the expenditure of scarce resources by pursuing the most profitable locations and minimizing misallocation of said resources when opening a new store. This will lead to better investment, decreased waste, and more reliable financial evaluations.

## EDA Objectives

As well as gaining a greater holistic understanding of the provided data, this EDA aims to answer the following questions:\

-   What preprocessing/cleaning is required?\
    -   What is the scale of missing values? How should they be handled?\
    -   Do erroneously encoded categorical variables need to be corrected?\
    -   How do date attributes need to be standardized or formatted to ensure accurate seasonality calculations?\
    -   Is the existing data "tidy"?\
        -   Each variable has its own column\
        -   Each observation has its own row\
        -   Each value has its own cell\
-   Which features are most correlated with each of the target sales metrics?\
-   What explanatory variables are collinear and how should that be handled?\
-   What affect does seasonality have on the target variables?\



# Loading packages and data 

```{r}
# Load packages 
library(lemon)
library(skimr)
library(lubridate)
library(magrittr)
library(zoo)
library(tidyverse)
library(readxl)
library(scales)
library(GGally)
library(ggrepel)
library(ggforce)
library(caret)
library(ggTimeSeries)
library(ggpointdensity)
library(fpp3)
library(patchwork)
library(plotly)

# Load data 
mvts <- read_csv("../data/time_series_data_msba.csv") %>%
  # removing unnamed row index column
  select(-1) %>% 
  # simplifying column names
  rename_with(~str_split_1(paste0("open_date,date,week_id,day_name,holiday,",
                                  "day_type,inside_sales,food_service,diesel,",
                                  "unleaded,site_id"), ",")) %>% 
  # set site_id as first column
  relocate(site_id) %>%
  arrange(site_id, date)

mvq <- read_csv("../data/qualitative_data_msba.csv") %>%
  # removing unnamed row index column
  # `RV Lanes Fueling Positions` and `Hi-Flow Lanes Fueling Positions` are duplicated columns
  select(-c(1, `RV Lanes Fueling Positions`, `Hi-Flow Lanes Fueling Positions`)) %>%
  # set site_id as first column
  select(site_id = site_id_msba, colnames(.)) %>% 
  # simplify column names
  rename_with(\(x){
    # replace spaces, slashes, and hyphens with underscores
    gsub(" +|-|/", "_", tolower(x)) %>%
      # remove single quotes and apostrophes
      gsub("'|â€™", "", .) %>%
      # validate variables beginning with numbers
      gsub("^(\\d)", "x\\1", .)
  }) %>%
  # distinguish from variable found in time series data
  rename(has_diesel = diesel)

```

# Initial inspection

## Time series data

```{r}
# First look
mvts %>%
  head(5)

```

```{r render = knitr::normal_print}
# Skim
mvts %>%
  mutate(site_id = as.character(site_id)) %>%
  skim() 
```


The time series data contains the daily, store level sales of the four target variables: `inside_sales`, `food_service`, `diesel`, and `unleaded`. Other data pertaining to dates are provided herein, including:\

-   `week_id`: Fiscal week number\
-   `day_name`: Day of the week name\
-   `open_date`: Date the store opened\
-   `holiday`: What holiday (if any) occured on that day\
-   `day_type`: Either "WEEKDAY" or "WEEKEND"\

None of the variables have any missing values that will have to be dealt with.

## Qualitative data

```{r}
# First look
mvq %>%
  head(5)

```

```{r render = knitr::normal_print}
# Skim
mvq %>%
  mutate(site_id = as.character(site_id)) %>%
  skim() 
```

The qualitative data set offers a mix of 52 categorical and numerical variables describing site attributes and demographic/socioeconomic statistics of the surrounding area. Interestingly, site #23065 is missing from this data set and will have to be removed from the time series data set.

```{r}
symdiff(mvts$site_id, mvq$site_id)
```

`mvq` does not have any explicitly `NA` values, but does utilize character string to indicate such. 


```{r render = knitr::normal_print}
mvq %>%
  # select columns containing "N/A" or "none"
  select(where(~any(grepl("^N/?A$|^none$", ., ignore.case = TRUE)))) %>%
  # for each column, calculate proportion of observations containing "N/A" or "none"
  summarise(
    across(
      everything(), 
      ~sum(grepl("^N/?A$|^none$", ., ignore.case = TRUE)) / n()
    )
  ) %>%
  # Covert to long-form
  pivot_longer(everything(),
               names_to = "col",
               values_to = "prop_na")
```

This can be solved with the following:

```{r render = knitr::normal_print}
mvq1 <- mvq %>%
  mutate(
    across(
      where(~any(grepl("^N/?A$|^none$", ., ignore.case = TRUE))),
      ~replace(., grepl("^N/?A$|^none$", ., ignore.case = TRUE), NA)
    )
  )

# Confirm same proportions as before
mvq1 %>% 
  # select columns containing missing values
  select(where(~any(is.na(.)))) %>%
  # for each column, calculate proportion of missing values
  summarise(
    across(
      everything(),
    ~sum(is.na(.)) / n()
    )
  ) %>%
  #convert to long-form
  pivot_longer(everything(),
               names_to = "col",
               values_to = "prop_na")
```

`mvq` also contains some zero-variance variables which would not contribute to any model and should be taken out.

```{r}
mvq1 %>%
  summarise(
    across(
      everything(),
      list(
        # for each column, calculate number of distinct values
        ndistinct = ~n_distinct(., na.rm = TRUE),
        # for each column, calculate variance
        var = ~var(., na.rm = TRUE) %>% round(4),
        # for each column, concatenate up to three unique values
        samp = ~paste0(na.omit(unique(.)[1:3]), collapse = ", ")
      )
    )
  ) %>%
  #convert to long form
  pivot_longer(everything(),
               names_to = c("col", ".value"),
               names_pattern = "(.*)_(.*)") %>%
  arrange(ndistinct)


```

It also appears some of the pump data is contradictory.

```{r}
sk_list <- list(
  c(
    "hi_flow_lanes",
    "hi_flow_lanes_layout"
  ),
  c(
    "hi_flow_lanes_layout",
    "rv_lanes"
  ),
  c(
    "rv_lanes",
    "rv_lanes_layout"
  ),
  c(
    "rv_lanes_layout",
    "hi_flow_rv_lanes"
  ),
  c(
    "hi_flow_rv_lanes",
    "hi_flow_rv_lanes_layout"
  ),
  c(
    "hi_flow_rv_lanes_layout",
    "rv_lanes_fueling_positions"
  ),
  c(
    "rv_lanes_fueling_positions",
    "hi_flow_lanes_fueling_positions"
  )
)

sk <- lapply(sk_list,
       \(x){
         mvq %>%
           select(contains("lanes")) %>%
           mutate(across(everything(), as.character)) %>%
           count(
             pick(all_of(x))
           ) %>%
           mutate(id = row_number(),
                  .before = 1) %>%
           pivot_longer(-c(n, id)) %>%
           unite("name",
                 name:value,
                 sep = "\n") %>%
           group_by(id) %>%
           mutate(name2 = name[2]) %>%
           ungroup() %>%
           filter(name != name2)
       }) %>%
  list_rbind

all_nodes <- unique(c(as.character(sk$name), as.character(sk$name2)))

sk1 <- sk %>%
  mutate(from_id = match(name, all_nodes) - 1,
         to_id = match(name2, all_nodes) - 1)

plot_ly(
  type = "sankey",
  domain = list(
    x = c(0,1),
    y = c(0,1)
  ),
  orientation = "h",
  #valueformat = ".0f",
  node = list(
    pad = 15,
    thickness = 20,
    line = list(
      color = "black",
      width = 0.5
    ),
    label = all_nodes
  ),
  link = list(
    source = sk1$from_id,
    target = sk1$to_id,
    value = sk1$n
  )
) %>% 
  layout(
    title = "Sankey Diagram",
    font = list(
      size = 10
    )
  )
```


```{r}
# Applying all basic cleaning steps and assigning to original object name
mvts <- read_csv("../data/time_series_data_msba.csv") %>%
  # removing unnamed row index column
  select(-1) %>% 
  # simplifying column names
  rename_with(~str_split_1(paste0("open_date,date,week_id,day_name,holiday,",
                                  "day_type,inside_sales,food_service,diesel,",
                                  "unleaded,site_id"), ",")) %>% 
  # set site_id as first column
  relocate(site_id) %>%
  arrange(site_id, date) %>%
  # removing store not found in `mvq`
  filter(site_id != 23065)

mvq <- read_csv("../data/qualitative_data_msba.csv") %>%
  # removing unnamed row index column
  # `RV Lanes Fueling Positions` and `Hi-Flow Lanes Fueling Positions` are duplicated columns
  select(-c(1, `RV Lanes Fueling Positions`, `Hi-Flow Lanes Fueling Positions`)) %>%
  # set site_id as first column
  select(site_id = site_id_msba, colnames(.)) %>% 
  # simplify column names
  rename_with(\(x){
    # replace spaces, slashes, and hyphens with underscores
    gsub(" +|-|/", "_", tolower(x)) %>%
      # remove single quotes and apostrophes
      gsub("'|â€™", "", .) %>%
      # validate variables beginning with numbers
      gsub("^(\\d)", "x\\1", .)
  }) %>%
  # distinguish from variable found in time series data
  rename(has_diesel = diesel) %>%
  # creating explicit NA's
  mutate(
    across(
      where(~any(grepl("^N/?A$|^none$", ., ignore.case = TRUE))),
      ~replace(., grepl("^N/?A$|^none$", ., ignore.case = TRUE), NA)
    )
  ) %>%
  # removing zero-variance variables
  select(-c(c(front_door_count, godfathers_pizza, has_diesel,
  car_wash, ev_charging, rv_lanes_stack_type,
  hi_flow_lanes_stack_type, hi_flow_rv_lanes_stack_type,
  non_24_hour, self_check_out)))
```


# Dates

The entire data set spans from 2021-01-12 to 2023-08-16 and all 38 stores are present for one year and one day. Is the extra day of any analytical significance or simply an artifact of the fiscal calendar? Given that every store exists for the same length of time, it will be helpful to know the distribution of stores across dates.

```{r}
# mvts %>%
#   group_by(date) %>%
#   # count of stores for each date
#   summarise(n = n()) %>%
#   ggplot() +
#   geom_col(aes(date, n),
#            fill = "darkred", color = "darkred") +
#   # Fix date axis labels
#   scale_x_date(breaks = seq(as_date("2021-01-01"), as_date("2023-09-01"), "3 months"),
#                labels = ~ifelse(month(.) == 1, format(., "%Y"), format(., "%b"))) +
#   scale_y_continuous(breaks = seq(0, 30, 5)) +
#   theme_minimal(20) +
#   labs(title = "Count of stores for each date")

# Bar plot
{
  mvts %>%
  # count of stores for each date
  count(date) %>%
  ggplot() +
  geom_col(aes(date, n),
           fill = "darkred", alpha = 0.7, width = 1) +
  # Fix date axis labels
  scale_x_date(breaks = seq(as_date("2021-01-01"), as_date("2023-09-01"), "3 months"),
               labels = ~ifelse(month(.) == 1, format(., "%Y"), format(., "%b"))) +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  theme_minimal(20) +
  labs(title = "Count of stores vs date")
} / # vertically combining plots with `patchwork` package
  # Line plot
  {
    mvts %>%
      arrange(open_date, site_id, date) %>% 
      # rescale site_id for plot
      mutate(site_id = consecutive_id(site_id)) %>%
      ggplot() +
      geom_line(aes(date, site_id, group = site_id),
                color = "steelblue", linewidth = 2, show.legend = FALSE) +
      # Fix date axis labels
      scale_x_date(breaks = seq(as_date("2021-01-01"), as_date("2023-09-01"), "3 months"),
                   labels = ~ifelse(month(.) == 1, format(., "%Y"), format(., "%b"))) +
      scale_y_continuous(breaks = seq(0, 38, 4),
                         minor_breaks = NULL) +
      theme_minimal(20) +
      labs(title = "Store-wise date range",
           y = "Cumulative store count")
  }

```

Given that `open_date` is not uniformly distributed, network-wide seasonality will have to be calculated either on a sales per store basis or by standardizing the date by attributing a day ID similar to `week_id`. Maverik expressed the importance of aligning days in a standardized manner, which is why `week_id` is included. I'll begin dissecting the fiscal calendar structure by determining if a singular day of the week begins each week.

```{r render = knitr::normal_print}
mvts %>%
  distinct(date, .keep_all = TRUE) %>%
  # since week_id resets each year, values are not unique to distinct weeks
  group_by(unique_week_id = consecutive_id(week_id)) %>%
  # remove incomplete weeks
  mutate(unique_week_len = n()) %>%
  filter(unique_week_len == 7) %>%
  # subset first day of fiscal week
  summarise_all(first) %>%
  # determine variety 
  count(day_name)
```

Now that we know each (complete) week begins on Friday, we can begin constructing a standardized day_id.

```{r}
# Begin with completed date range found in data
day_id_df <- tibble(date = seq(as_date("2021-01-01"), as_date("2023-12-31"), "1 day")) %>%
  # Calculate week_id
  mutate(week_id = yearweek(date, week_start = 5) %>% format("%V") %>% as.numeric(),
         # since the first day of fiscal year 2022 is actually in 2021, special logic must be 
         # applied to identify the beginning of the year
         x = case_when(lag(week_id, default = 52) == 52 & week_id == 1 ~ 1),
         year = 2020 + rollapplyr(x, width = n(), FUN = sum, na.rm = TRUE, partial = TRUE)) %>%
  group_by(year) %>%
  mutate(day_id = row_number()) %>%
  select(-x) %>%
  ungroup()

day_id_df

# Validating accuracy by comparing to mvts' week_id
day_id_df %>%
  # trimming to mvts' date range
  filter(date >= "2021-01-12",
         date <= "2023-08-16") %>%
  # counting instances of manually calculated week_id and renaming columns
  count(custom_week_id = week_id,
        name = "custom_n") %>%
  bind_cols(mvts %>%
  distinct(date, week_id) %>%
  # counting instances of native week_id and renaming columns
  count(mvts_week_id = week_id,
        name = "mvts_n")) %>%
  # subsetting discrepant rows
  filter(custom_n != mvts_n)


```

# Explanatory variables

```{r}
mvq
```


# Target variables

```{r}
# Histogram (density)
mvts %>%
  pivot_longer(c(inside_sales, food_service, diesel, unleaded),
               names_to = "metric",
               values_to = "sales") %>%
  group_by(metric, site_id) %>%
  summarise(sales = sum(sales)) %>%
  mutate(avg = mean(sales),
         med = median(sales)) %>%
  ggplot() +
  geom_density(aes(sales, color = metric),
               linewidth = 1) +
  #coord_cartesian(xlim = c(0, 2e6)) +
  theme_minimal(20) +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  labs(title = "Distribution of key sales metrics")

# Violin
mvts %>%
  pivot_longer(c(inside_sales, food_service, diesel, unleaded),
               names_to = "metric",
               values_to = "sales") %>%
  group_by(metric, site_id) %>%
  summarise(sales = sum(sales)) %>%
  #mutate(sales = scale(sales)[,1]) %>%
  ggplot() +
  geom_violin(aes(metric, sales, fill = metric),
              color = "white", draw_quantiles = c(0.25, 0.5, 0.75),
              scale = "area") +
  theme_minimal(20) +
  labs(title = "Distribution of key sales metrics (scaled and centered)")

# Line
mvts %>%
  # add `day_id`
  left_join(day_id_df %>%
              select(date, day_id), "date") %>%
  # place all sales into one column, labeled by another
  pivot_longer(c(inside_sales, food_service, diesel, unleaded),
               names_to = "metric",
               values_to = "sales") %>%
  # aggregate by `day_id` and sales metric
  group_by(metric, day_id) %>%
  summarise(sales = sum(sales),
            day_type = day_type[1]) %>%
  ggplot() +
  # `group = 1` prevents a separate line for each `day_type`
  geom_line(aes(day_id, sales, color = day_type, group = 1)) +
  # create separate plot for each `metric`
  facet_rep_wrap(~metric, repeat.tick.labels = TRUE, scales = "free", ncol = 1) +
  theme_minimal(20) +
  theme(legend.position = "top",
        legend.title = element_blank())

# Line with holiday 

# Categorical line color
mvts %>%
  # add `day_id`
  left_join(day_id_df %>%
              select(date, day_id), "date") %>%
  group_by(day_id) %>%
  mutate(is_holiday = case_when(holiday != "NONE" ~ "holiday",
                                .default = "regular day")) %>%
  # place all sales into one column, labeled by another
  pivot_longer(c(inside_sales, food_service, diesel, unleaded),
               names_to = "metric",
               values_to = "sales") %>%
  # aggregate by `day_id` and sales metric
  group_by(metric, day_id) %>%
  summarise(sales = sum(sales),
            is_holiday = is_holiday[1]) %>% 
  ggplot() +
  # `group = 1` prevents a separate line for each `is_holiday`
  geom_line(aes(day_id, sales, color = is_holiday, group = 1),
            linewidth = 1) +
  # Create generalized date labels
  scale_x_continuous(breaks = c(1, cumsum(days_in_month(as_date(paste0("2021-", month.abb, "-01")))) + 1),
                     labels = ~format(as_date("2021-12-31") + ., "%b %d")) +
  # Rescale labels to thousands
  scale_y_continuous(labels = ~./1e3) +
  # create separate plot for each `metric`
  facet_rep_wrap(~metric, repeat.tick.labels = TRUE, scales = "free", ncol = 1) +
  theme_minimal(20) +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  labs(title = "Annualized store sales",
       x = "generalized date",
       y = "sales $ (thousands)")

# Create df for plot to shade holidays
holiday_4gg <- mvts %>%
  # add `day_id`
  left_join(day_id_df %>%
              select(date, day_id), "date") %>%
  # place all sales into one column, labeled by another
  pivot_longer(c(inside_sales, food_service, diesel, unleaded),
               names_to = "metric",
               values_to = "sales") %>%
  # aggregate by `day_id` and sales metric
  group_by(metric, day_id) %>%
  mutate(sales = sum(sales)) %>%
  # aggregate by `metric`
  group_by(metric) %>%
  mutate(sales_max = max(sales),
         sales_min = min(sales)) %>%
  # Keep only holidays
  filter(holiday != "NONE") %>%
  group_by(holiday, metric) %>%
  # get min/max of `day_id` and `sales`
  reframe(day_id = range(day_id),
          sales = c(sales_min[1], sales_max[1]))

mvts %>%
  # add `day_id`
  left_join(day_id_df %>%
              select(date, day_id), "date") %>%
  # place all sales into one column, labeled by another
  pivot_longer(c(inside_sales, food_service, diesel, unleaded),
               names_to = "metric",
               values_to = "sales") %>%
  # aggregate by `day_id` and sales metric
  group_by(metric, day_id) %>%
  summarise(sales = sum(sales),
            day_type = day_type[1]) %>%
  ggplot() +
  geom_mark_rect(data = holiday_4gg,
                 aes(day_id, sales, 
                     group = interaction(holiday, metric)),
                 expand = unit(1.5, "mm"), radius = unit(0.5, "mm"), 
                 fill = "#000000", alpha = 0.1, color = NA) +
  # `group = 1` prevents a separate line for each `day_type`
  geom_line(aes(day_id, sales, color = day_type, group = 1)) +
  # Rescale labels to thousands
  scale_y_continuous(labels = ~./1e3) +
  # create separate plot for each `metric`
  facet_rep_wrap(~metric, repeat.tick.labels = TRUE, scales = "free", ncol = 1) +
  theme_minimal(20) +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  labs(title = "Sales vs day_id",
       subtitle = "Shaded regions indicate holidays",
       y = "sales $ (thousands)")

# Correlation
mvts %>%
  left_join(day_id_df %>%
              select(date, day_id), "date") %>%
  select(inside_sales, food_service, diesel, unleaded) %>%
  ggpairs() +
  labs(title = "Generalized pairs of daily store sales")

mvts %>%
  left_join(day_id_df %>%
              select(date, day_id), "date") %>%
  group_by(site_id) %>%
  summarise(across(c(inside_sales, food_service, diesel, unleaded), sum)) %>%
  ggpairs() +
  labs(title = "Generalized pairs of annualized store sales")
  
```

Based on the time series line plot, it's clear that there is weekly and yearly seasonality but weekly seasonality is more pronounced with weekdays seeing greater sales than weekends. We also see that certain holidays affect sales more than others. This is not surprising but this will certainly have to be accounted for during modeling.

All of the sales metrics besides `diesel` are fairly normally distributed but still right-skewed. Intuitively, `inside_sales` and `food_service` are very positively correlated. The relationship of the other pairs are rather heteroskedastic or otherwise inconsistent. Determining which (if any) features in the available data can explain this irregular variance will be valuable when forecasting.   

The scatter plot from the generalized pairs plot reveals that some locations had no sales on at least one day. One may think that these might be stores not open 24 hours a day, but we discovered earlier that only one value for `non-24_hour` exists in the data and subsequently all stores are open 24 hours a day. 

```{r}
mvts %>%
  group_by(site_id) %>%
  filter(if_any(c(inside_sales, food_service, diesel, unleaded), ~any(.<=0))) %>%
  summarise(across(c(inside_sales, food_service, diesel, unleaded),
                   list(avg = ~mean(.),
                        med = ~median(.),
                        sd = ~sd(.),
                        max = ~max(.),
                        min = ~min(.),
                        dec1 = ~quantile(., 0.1),
                        daysNoSales = ~sum(. == 0)))) %>%
  pivot_longer(-site_id,
               names_to = c("metric", ".value"),
               names_pattern = "(.*)_(.*)")
```

We can determine that only two stores account for the zero sales values. Based on each stores' summary statistics, the absence of sales for even a single day seems peculiar. This could be indicative of some disruption to the facilities that is not indicated in the data. It can be inferred that other stores could have possibly also experienced similar, but not complete, disruptions that would affect their sales that we cannot account for. Unfortunately, not much can be done about this and hopefully the scale of such instances are minor.

Conversely, there seems to be at least one store that has dramatically higher sales than the rest which begs the question of what to do with sufficiently outlying data. Before deciding to remove such cases, it will be important to see if the other corresponding data can suggest an explanatory pattern which is valuable for prediction.

```{r}
mvts %>%
  # distinguish unusual diesel seller
  mutate(big_seller = site_id == 21980,
         alpha = ifelse(big_seller, 1, 0.4)) %>%
  arrange(big_seller) %>%
  ggplot() +
  geom_point(aes(inside_sales, diesel, color = big_seller, alpha = alpha)) +
  # dynamic opacity
  scale_alpha_identity() +
  theme_minimal(20) +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  labs(title = "Diesel vs Inside Sales, spotlight on Site 21980")
```



## Relationship with features

```{r}
# Numerical features
corr_df <- mvts %>%
  group_by(site_id) %>%
  summarise(across(inside_sales:unleaded, sum)) %>%
  left_join(mvq, "site_id") %>%
  select(where(is.numeric), -c(site_id, open_year, years_since_last_project))

corr_df %>%
  cor() %>%
  ggcorrplot::ggcorrplot(#method = "circle",
                         type = "lower",
                         p.mat = ggcorrplot::cor_pmat(corr_df),
                         lab = TRUE,
                         lab_size = 3,
                         outline.color = "white",
                         hc.order = TRUE,
                         insig = "blank")
```































